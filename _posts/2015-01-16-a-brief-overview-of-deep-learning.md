---
layout: post
title: "深度学习简明介绍"
description: ""
category: [Deep Learning]
---

本文翻译自 [A Brief Overview of Deep Learning][16]。如果对神经网络感兴趣，阅读 [如何简单形象又有趣地讲解神经网络是什么？][13] 这个问题下面的答案会很有帮助，强烈推荐。

（这是来自 [Ilya Sutskever][17] 的关于深度学习（Deep learning）以及实践建议的约稿。感谢 Ilya 为本文付出的努力。）

最近 [Deep Learning][18] 真的很流行，各种大大小小的公司都投入其中并获利，发展如火如荼，并且取得了一些成就，比如：`large deep neural networks` （[DNN][19]）在下面的领域建树颇丰：

- 语音识别
- 视觉对象识别
- 一些语言相关的任务，比如机器翻译以及语言模型

于是问题来了：

- 深度学习到底神在哪里？（后面，我们将使用 `large deep neural networks` - __LDNN__，我们平常谈到深度学习时主要说的就是这个）
- 为什么 __LDNN__ 现在才火起来，它和之前的神经网络有什么不同？

总得来说，如果我们想要训练一个 __LDNN__，坊间传言是很难的，需要多年经验积累的 “黑魔法”。当然具备一定的经验是有帮助的，但是其中必需的技能还是出乎很多人意料得有限，中间也只需要注意少量的坑而已。而且，我们已经有很多最先进的开源的神经网络实现（比如：[Caffe][20],[cuda-covnet][21],[Torch][22],[Theano][23]），这让我们学习如何实现一个神经网络并使其工作变得很简单。

## 深度学习为什么有用？

我们都知道：工欲善其事，必先利其器。为了解决困难的问题，我们需要强大的模型，否则就没有可能解决面临的问题，而和我们使用的算法是否足够好没有关系。另外一个必要条件是这个模型是可以被训练的，否则模型的作用就受到很大限制，无法应对潜在的变化。

幸运的是，__LDNN__ 足够强大而且可被训练。

## LDNNs 为什么强大？

当我提到 __LDNNs__ 时，其实我说的是 10-20 层神经网络（这是其可被今天的算法训练的基础）。接下来我就来从下面几个方面揭示 __LDNNs__ 可以如此强大的原因：

- 常规统计模型学习的是简单模式和聚合，而 __LDNNs 学习的是计算方法，包含一定数量步骤的大规模并行计算。__  
业界普遍认为任何算法都可以被转化为相应深度的回路计算（算法执行的每一步为一层————[例子][24]）而且这里的深度越深，将算法转化为回路计算的成本越高。神经网络也是回路，更深的神经网络可以实现更复杂的算法————这就是为什么深度是能力的代名词。
	- 注意：这里可以比较容易看到一个神经网络中的神经元可以通过简单设置计算其输入的关联性以及非关联性。

- 出乎我们意料的是，__神经网络要比基于布尔判断的流程控制（ [boolean circuits][25] ）更有效率。__ 这里我所说的更有效率，是指一个较浅的神经网络就可以解决需要非常多层 `boolean circuits` 才能处理的问题。举个让人难以置信的例子吧，关于一个 2 层的深度神经网络和可以对 N 个 N-bit 数字进行排序的相应规模的计算单元。由于我刚开始听到这个结论时也被惊到了，所以，我实现了一个简单的神经网络并训练它对 10 个 6-bit 的数字进行排序，过程出乎意料的简单。如果使用 `boolean circuits` 的话就不可能如此简单。
	- 这里神经网络效率高的原因在于神经元执行了分流操作，这对于一个小型的布尔回路是不可能做到的。
	- 总得来说，人体神经元是比较慢的，然而人却可以在不到 1 秒的时间内完成很多复杂的任务。科学研究表明，人体神经元传导电信号的频率不会超过 100次/秒。这意味着如果人可以在 0.1 秒内解决一个问题，那么我们的神经元可以传导不超过 10 次的信号，那我们好像可以据此推导出：一个拥有 10 层的神经网络可以在 0.1 秒内做任何一个人在同样时间内可以做的事情。
	- 遗憾的是，上面推导出来的结论不是科学的结论。可以想象原因在于人体神经元要比神经网络中的人造神经元强大多了，虽然在一些情况下人造神经元会比人体神经元更强大（比如数值运算）。无论如何，上面的推论都是一个合理的假设。
- 有意思的是人可以在 0.1 秒之内解决很多复杂的认知问题，比如：人可以非常快速地识别面前的物体、通过面貌认出人、以及理解别人说的话。事实上，哪怕整个世界上只有一个人有这样处理复杂任务的能力，也可以作为 __LDNNs__ 可以做相同事情的有力证据————只要这个神经网络的连接关系被合理地设置。
- __但是神经网络不是需要做得很大规模吗？__ 也许吧，我们当然知道它们的规模不需要被做得及其大————就像我们的大脑就不是很大，而且人体神经元传递信号时会出现噪声时（举个例子），这意味着完成仅需一个人造神经元就能处理的任务需要很多人体神经元参与才能完成。这样完成处理所需时间超过 0.1 秒任务的 __LDNNs__ 中需要的人造神经元的数量会减少很多。

上面的讨论表明了（我个人觉得很有有力地表明了）对于一个非常广泛的问题来说，是存在一个能否解决它的被合理连接的 __LDNNs__ 的。关键的是，完成这个任务所需的计算单元的数量是很有限的————一般情况下由于需求有限，我们可以使用已有的硬件训练神经网络来获得处理该类问题的最好性能。这是我觉得最后一个很重要需要再阐述一下的点：

- 我们知道大多数机器学习算法是稳定的。也就是说，只要有足够的数据就能解决问题。这就带来了一个问题：稳定性一般情况下要求及其大的数据量。举个例子，[nearest neighbor algorithm][26] 算法能通过保存对所有预期输入的处理结果来解决任何问题。这对矢量支持机（support vector machine）也是成立的————对于很难的问题而言，我们需要对每个训练 case 进行矢量支持。这点对于仅有一层的神经网络也是成立的。如果我们对每个可能的训练 case 都有一个人造神经元来处理，这个神经元之对该训练 case 作出反应，那么我们就可以学习以及表现每一个从输入到输出的可能的处理逻辑。这样任何问题都可以在资源无限的情况下被解决，当然这对于我们有限的物理资源来说是不可接受的。
- 这里我们讨论的观点也是将 __LDNNs__ 区别于之前方法的关键。我们有理由确信一个合理规模的 __LDNNs__ 可以在解决各种需要我们处理的广泛问题的时候取得很好效果的。如果一个问题可以被人脑在 1 秒内解决，那我们就可以使用一个规模不大的神经网络高性能地解决这个问题。
- 但是我必须承认事先预测一个问题是否可以被一个深度神经网络解决是不可能的，尽管我们通常知道什么时候能确定一个类似的问题是否可以被一个规模可控的 __LDNN__ 处理。

这就是深度学习，给定一个问题，比如视觉对象识别，我们需要做的就是训练一个具有 50 层的 [convolutional neural network][27]。一个如此规模的神经网络是可以通过合理配置来达到人脑在视觉对象识别上的效率的。接下来我们就需要知道怎样做，然后做出来，解决问题。

## 学习

什么是学习呢？学习的过程就是找到神经网络权重的设置，使其能从训练数据中获得最好的结果。换句话说，就是我们将标签化的数据作为参数 `推送` 给神经网络，使其完成这个学习的过程。

深度学习的成功基于一个幸运的事实：良好调校并且审慎初始化的 __随机梯度下降__（[stochastic gradient descent(SGD)][28]）方法可以针对实际问题对 __LDNNs__ 进行训练。这不是个简单的事实，因为一个神经网络的训练错误是其权重的方程，并且该方程高度 `非凸` （Non-convex），当对其进行 `非凸优化` 之时，经验告诉我们肯定竹篮打水一场空。只有 `凸函数` 是好结果，`非凸函数` 是坏结果。而且，一般情况下，__SGD__ 针对我们关心的问题对 __LDNNs__ 进行训练非常擅长。训练神经网络的问题在于 [NP-hard][29]，而且实际上存在一个数据集族使得包含 3 个隐藏单元的神经网络的学习变得 __NP-hard__，然而 __SGD__ 可以在实践中解决这个问题，这也是深度学习的支柱。

我们可以相当确信地说成功地训练一个神经网络的基础在于数据之间的关联性，这使得学习过程能自我优化并处理数据之间更复杂的关联性。我曾经做过一个实验来证明这一点：我之前发现训练神经网络去解决 __奇偶校验问题__ （[parity problem](http://en.wikipedia.org/wiki/Parity_problem_(sieve_theory))）很困难，我能成功地训练神经网络来对 25bit，29bit 的整数进行奇偶校验，但是对于 31bit 的整数却从没有成功过（顺便说一下，我不是说对于 30bit 以上的整数进行奇偶校验的学习是不可能的，我只是说我没有成功做到）。现在，我们知道奇偶校验是高度不稳定的，因为这个问题没有线性关联性：输入的每个线性函数和输出完全没有关系，这对于神经网络来说是个问题，因为神经网络基本上会被初始化为线性的（所以可能我不应该使用很大的初始权重？我会在本文后面讨论权重初始化）。所以我的假设（很多其他科学家也认同这样的假设）是认为神经网络从注意到大部分输入和输出之间明显的关联性开始学习的过程————无论简单还是困难，然后这个网络从一个关联性跳到一个更复杂的关联性，类似于机会主义登山者（opportunistic mountain climber）

## 概括

虽然谈论神经网络优化的内在具体需求很困难（除非就说最简单的情况，所有问题都只有一个极点值而且没有乐趣可言），我们还是可以从大的方面谈谈概况。

[Valiant][31] 在他 1984 年发表的论文 "A Theory of the Learnable" 中证明了粗略地讲，如果存在有限数量的函数，比如 N 个，那么在拥有超过 `log(N)` 个训练 case 乘以一个很小的常数因子的情况下每个训练错误将接近于每个测试错误。很明显在这种情况下，[过度拟合（overfitting）][32] 基本上是不可能的（过度拟合产生于当训练错误和测试错误之间有较大差距时，有人告诉我这个结果在 [Vapnik’s book][33] 中以一个练习的形式出现过）。这个定理很容易证明，不过这不是本文的重点。

这个结果对于任何神经网络的实现中具有重要的意义。假设我有一个具有 N 个参数的神经网络，每个参数是一个 32bit 浮点数，那么这个神经网络就被特化为 32N bit，这意味着我们将拥有不多于 2 的 32N 次方的不同神经网络，而且可能远少于这个数。这种情况下，当我们拥有多于 32N 个训练 case 的时候我们将不会过度拟合很多。这点很好，意味着理论上计算参数是可以的，而且如果我们很确信每个权重只需要 4 bits（假设情况） 的时候，那么除此之外就是噪声了，我们也就可以相当确信训练 cases 的数量将会是 4N 而不是 32N 的常数因子。

## 总结

如果我们想解决一个困难的问题我们可能需要一个含有很多参数的 __LDMM__。然后我们需要一个大规模高质量标签化的训练集来保证这个神经网络拥有足够的信息定义所有网络的连接性，获得这个训练集后，我们需要对其使用 __SGD__ 直到这个神经网络解决了问题，如果这个网络足够大规模并且足够深，那通常是可以解决问题的。

## 实践建议

翻译原文的目的在于了解深度学习的概况，原文中的实践建议给出了很多条 best practice，由于目前缺少实践经验就不翻译了。

## 深度学习资源

- [牛人的deep learning学习笔记汇总](http://blog.csdn.net/denghp83/article/details/8968579#0-tsina-1-82836-397232819ff9a47a7b7e80a40613cfe1)
- [又一位牛人的Machine Learning实验笔记](http://blog.csdn.net/denghp83/article/details/8996662#0-tsina-1-56037-397232819ff9a47a7b7e80a40613cfe1)
- [机器学习自学指南](http://blog.jobbole.com/58937/)
- [最流行的4个机器学习数据集](http://www.jianshu.com/p/be23b3870d2e)
- [机器学习算法：决策树](http://boytnt.blog.51cto.com/966121/1569763)
- [机器学习代码心得之​有监督学习的模块](http://weibo.com/p/1001603795687165852957?sudaref=ml.memect.com)
- [机器学习日报](http://ml.memect.com/)
- [Awesome Big Data](https://github.com/onurakpolat/awesome-bigdata)
- [Deep Learning Tutorial](https://www.kaggle.com/c/facial-keypoints-detection/details/deep-learning-tutorial)
- [awesome-deep-learning](https://github.com/Ashara12/awesome-deep-learning)
- [deeppy](https://github.com/andersbll/deeppy)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [A Deep Dive into Recurrent Neural Nets](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)
- [Deep Learning on Hadoop 2.0](https://www.paypal-engineering.com/2015/01/12/deep-learning-on-hadoop-2-0-2/)

### 知乎上面关于神经网络讨论比较好的问题：

话题和问题是一方面，从中可以找到很多对深度学习感兴趣的人或者业内资深人士并关注是另一方面。

- [知乎话题：深度学习（Deep Learning）][14]
- [知乎话题：神经网络（Neural Networks）][15]
- [人工智能、模式识别领域最终是否会被一种本质、通用的算法主导？][1]
- [模式识别、机器学习、神经网络、(泛函分析？)的学习顺序][2]
- [神经网络激励函数的作用是什么？有没有形象的解释][3]
- [如何理解神经网络里面的反向传播算法？][4]
- [有没有好理解的关于神经网络的书推荐？][5]
- [数据分析/挖掘工作的疑惑？][6]
- [学习神经网络、SVM等机器学习的知识，为了更好的投入到应用当中，用matlab还是c++好呢？][7]
- [神经网络的收敛性与稳定性？][8]
- [「深度神经网络」（deep neural network）具体是怎样工作的？][9]
- [为什么人工智能的研究都是基于算法，而不是基于“硬件”？][10]
- [机器学习有很多关于核函数的说法，核函数的定义和作用是什么？][11]
- [机器学习、统计分析、数据挖掘、神经网络、人工智能、模式识别之间的关系是什么？][12]
- [如何简单形象又有趣地讲解神经网络是什么？][13] - 非常好的问题和非常好的答案

[1]: http://www.zhihu.com/question/22933273
[2]: http://www.zhihu.com/question/23008559
[3]: http://www.zhihu.com/question/22334626
[4]: http://www.zhihu.com/question/24827633
[5]: http://www.zhihu.com/question/22410747
[6]: http://www.zhihu.com/question/21102339
[7]: http://www.zhihu.com/question/20499347
[8]: http://www.zhihu.com/question/27227041
[9]: http://www.zhihu.com/question/19833708
[10]: http://www.zhihu.com/question/23136639
[11]: http://www.zhihu.com/question/24627666
[12]: http://www.zhihu.com/question/20747381
[13]: http://www.zhihu.com/question/22553761
[14]: http://www.zhihu.com/topic/19813032
[15]: http://www.zhihu.com/topic/19607065
[16]: http://yyue.blogspot.hk/2015/01/a-brief-overview-of-deep-learning.html
[17]: http://www.cs.toronto.edu/~ilya/
[18]: http://deeplearning.net/
[19]: http://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks
[20]: http://caffe.berkeleyvision.org/
[21]: https://code.google.com/p/cuda-convnet/
[22]: http://torch.ch/
[23]: http://deeplearning.net/software/theano/
[24]: http://link.springer.com/article/10.1007/BF02579196#page-1
[25]: http://en.wikipedia.org/wiki/Boolean_circuit
[26]: http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
[27]: http://deeplearning.net/tutorial/lenet.html
[28]: http://research.microsoft.com/pubs/192769/tricks-2012.pdf
[29]: http://en.wikipedia.org/wiki/NP-hard
[30]: http://research.microsoft.com/en-us/um/people/adum/publications/2003-noise-tolerant_learning.pdf
[31]: http://people.seas.harvard.edu/~valiant/
[32]: http://en.wikipedia.org/wiki/Overfitting
[33]: http://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031


